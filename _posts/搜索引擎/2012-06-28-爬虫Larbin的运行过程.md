---
layout: post
categories:
- Programming Review
tags:
- 爬虫Larbin
---
---------


爬虫larbin运行过程
=====================



从图中可以看出Larbin分为
八大部分：分别为URL 库，DNS 解析，连接处理，I／O 复用，文件排重，robots 解析，解析html 文件和存储HTML 文件  

![此处输入图片的描述][1]

运行过程：  
1． 运行配置文件  
解析larbin．conf 文件，也就是自定义的配置文件，在此文件中会得到要抓取的种子节点将其存入URL 库中。  
URL 库模块的实现是通过PersistentFifo 类来完成的，其主要作用是存放新分析出来的URL。  
2． URL的DNS解析  
随后把需要进行DNS 解析的URL 从URL 库中调入内存，域名解析这一部分是通过NameSite 类来实现的。解析完毕后IP 地址回存到内存队列中  
3． 发起连接  
connexion类进行连接处理。  
If 初次请求 then某主机需要先请求robots 文件  
Else可以直接请求URL，对此类站点larbin 使用IPSite类进行处理，该类主要负责填写需要请求的URL 地址并对服务器发起请求报文  
4． I/O多路复用式请求  
套接字设置为非阻塞状态，这时larbin 不会等待应答而是选用了poll 系统函
数将连接设置为阻塞状态继续取出下一个conextions 对象填写请求报文发起http 请求。  
当所有对象取毕后，利用poll函数得出一组连接状态的屏蔽字，可以分辨出当前哪个连接已被激活，从而对当前激活连接进行处理。  
5． 文件排重  
处理应答报文时，首先对内容进行文件排重处理，larbin 使用hashDup 类来处理这一部分。  
If 重复 then 丢弃  
Else 使用file类进行解析             
6． 对内容进行解析  
File类处理对文件的解析会派生出如下两个类：  
Robots类，用于解析robots文件。爬虫必须遵守robots 协议，所以一般对一个新站点来说首次请求的是robots 文件，将对robots 文件进行解析。  
If 不想被抓取 放到记录禁止URL的forbidden数组中  
Else 进行解析HTML文件  
Html类：把新分析出来的URL 提取后将HTML 文件存储到外存上，这样一张完整的页面就抓取到  
1． 深度判断  
If  新分析出的URL 与该网页URL 拥有相同的域名，则将新URL 链接深度加1  
Else  将URL深度值置为初始值1  
2． URL过滤 查看URL是否可抓取  
通过与forbidden数组中的URL对比  
If 相同 then 不可抓取    
Else 随意抓取    
3． URL排重  
由于超文本文档结构的特殊性如果没有URL 排重处理的话，抓取到的网页冗余是不可估量的，通过hashTable 类对URL 进行排重处理。  
有两类的重复一是由于超链接的原因，很可能会出现不同的网页包含相同的URL，发生了URL 重复。二是由于服务器镜像的原因，会产生两个不同的网址指向相同的内容，发生了文档重复。  
通过以上三个环节的处理剩余的URL存入URL 库中作为扩展抓取的URL 源  
7． 存储HTML文件  


  [1]: http://hi.csdn.net/attachment/201203/16/0_1331873211xDZ2.gif